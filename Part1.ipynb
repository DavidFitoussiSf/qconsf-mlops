{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In Part1 of the project, we will develop and deploy a text classification ML model to predict news categories from the text of the news article:\n",
    "\n",
    "1. Train a text classification model. \n",
    "2. Evaluate the trained model performance on validation dataset. Explore tools and techniques such as behavioral/scenario-base testing to augment commonly used performance evaluation setups.\n",
    "3. Build a simple web application in Python that uses the trained model to do online inference, and test this application locally.\n",
    "4. [advanced] Deploy the application as a microservice in the cloud using AWS Lambda. \n",
    "\n",
    "Throughout the project there are default model & system architectures that are \"good defaults\" for building ML systems. That said, there's many different ways to configure and scale this setup! Suggestions always welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports that will be needed for this exercise\n",
    "\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "\n",
    "LABEL_SET = [\n",
    "    'Business',\n",
    "    'Sci/Tech',\n",
    "    'Software and Developement',\n",
    "    'Entertainment',\n",
    "    'Sports',\n",
    "    'Health',\n",
    "    'Toons',\n",
    "    'Music Feeds'\n",
    "]\n",
    "\n",
    "WORD_VECTOR_MODEL = 'glove-wiki-gigaword-100'\n",
    "EPS = 0.001\n",
    "SEED = 42\n",
    "\n",
    "DIRECTORY_NAME = \"data\"\n",
    "DOWNLOAD_URL = 'https://corise-mlops.s3.us-west-2.amazonaws.com/qconsf/dataset_part1.zip'\n",
    "\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Load Datasets\n",
    "\n",
    "[AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) is a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. The news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626). \n",
    "\n",
    "The dataset has the text description (summary) of the news article along with some metadata. **For this exercise, we will use a cleaned up subset of this dataset** \n",
    "\n",
    "Schema:\n",
    "* Source - News publication source\n",
    "* URL - URL of the news article\n",
    "* Title - Title of the news article\n",
    "* Description - Summary description of the news article\n",
    "* Category (Label) - News category\n",
    "\n",
    "Sample row in this dataset:\n",
    "```\n",
    "{\n",
    "    'description': 'A capsule carrying solar material from the Genesis space '\n",
    "                'probe has made a crash landing at a US Air Force training '\n",
    "                'facility in the US state of Utah.',\n",
    "    'id': 86273,\n",
    "    'label': 'Sci/Tech',\n",
    "    'source': 'Voice of America',\n",
    "    'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
    "    'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'\n",
    " }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from S3 bucket\n",
    "# Note: you can skip this if you've already downloaded the data locally\n",
    "\n",
    "http_response = urlopen(DOWNLOAD_URL)\n",
    "zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "zipfile.extractall(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into memory\n",
    "Datasets = {}\n",
    "for ds in ['training', 'validation']:\n",
    "    with open('data/{}.json'.format(ds), 'r') as f:\n",
    "        Datasets[ds] = json.load(f)\n",
    "    print(\"Loaded Dataset {0} with {1} rows\".format(ds, len(Datasets[ds])))\n",
    "\n",
    "print(\"\\nExample train row:\\n\")\n",
    "display(Datasets['training'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Take a look at `NewsCategoryClassifier` in `model/classifier.py` and familiarize yourself with the model architecure. \n",
    "\n",
    "In the interest of time, we are providing this default classification model implementation. \n",
    "This is a good default implementation of the classifier. But if you wish to extend/modify any part of this pipeline, or explore new model architectures you should definitely feel free to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model training & evaluation\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "X_test, Y_true = [], []\n",
    "\n",
    "for row in Datasets['training']:\n",
    "    X_train.append(row['description'])\n",
    "    Y_train.append(row['label'])\n",
    "\n",
    "for row in Datasets['validation']:\n",
    "    X_test.append(row['description'])\n",
    "    Y_true.append(row['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.classifier import NewsCategoryClassifier, WordVectorFeaturizer\n",
    "\n",
    "classifier = NewsCategoryClassifier(\n",
    "    config={\n",
    "        'word_vector_model': WORD_VECTOR_MODEL,\n",
    "        'word_vector_dim': 100\n",
    "    },\n",
    ")\n",
    "\n",
    "classifier.fit(X_train, Y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check with an example prediction\n",
    "\n",
    "print(f\"input:\\n{X_test[0]}\\n\\n\")\n",
    "print(f\"predictions:\\n{classifier.predict_proba(X_test[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Model performance evaluation for real-world ML applications can often be complex since different stakeholders (ML engineers, product managers, sales teams) care about the impact on different dimensions. In this exercise, we will evaluate the News category classification in the following ways:\n",
    "\n",
    "\n",
    "* **Aggregate performance metrics**: Using standard metrics to evaluate ML model performance such as accuracy, mean squared error, BLEU scores etc (depending on the ML task) is a necessary first step, but far from sufficient. It serves to filter out models that are clearly suboptimal and reduces risk of launching bad models\n",
    "\n",
    "* **Cohort/Slice-based performance metrics**: It is important to track model performance not just in aggregate but for important cohorts/slices of your traffic. For example, it is important to track the performance of your hate speech detection model not just in aggregate but for traffic from each country, language etc to understand the gaps in performance.\n",
    "  \n",
    "* **Qualitative evaluations**: Behavioral tests can be helpful to root-cause individual instances of model failures and yield helpful insights to improve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model predictions on the validation set\n",
    "\n",
    "Y_pred = [classifier.predict_label(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\" Overall accuracy: {accuracy_score(Y_true, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Compute the precision, recall and F1 score per-class for the validation set\n",
    "\n",
    "Hint: check out sklearn.metrics \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"NOT IMPLEMENTED YET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Plot a confusion matrix for the news category classification model trained above. \n",
    "You should have a NxN matrix, where: \n",
    "matrix(i, j) = number of instances in the test set where true_label = LABEL_SET[i] and pred_label = LABEL_SET[j]\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dummy confusion matrix, replace with the actual one\n",
    "cm = np.zeros((len(LABEL_SET), len(LABEL_SET)))\n",
    "\n",
    "# display confusion matrix -- no code change needed here\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABEL_SET)\n",
    "disp.plot(xticks_rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating confidence intervals of performance metrics with Bootstrap sampling\n",
    "\n",
    "When computing model performance metrics, the margin of error can vary a lot depending on the size of the test dataset. We are trying to estimate the model’s performance on any unseen data by empirically computing the model performance on the held-out validation set, and trying to quantify the confidence intervals in this estimation: \n",
    "\n",
    "If we were to measure the model’s performance on another independently collected validation dataset from the same underlying distribution, our model’s performance on this dataset is unlikely to be the same, but how different might they plausibly be? In this exercise, we will implement Bootstrap sampling to estimate the 95% confidence interval for perfornance metrics of the model:\n",
    "\n",
    "1. Generate N ‘bootstrap sample’ datasets, each the same size as the original validation set. Each bootstrap sample dataset is obtained by sampling instances uniformly at random from the original validation set (with replacement).\n",
    "2. On each of the bootstrap sample datasets, calculate the performance metric of choice.\n",
    "3. From Step (2), you will end up with N different values. Sort them.\n",
    "4. The 95% confidence interval is given by the 2.5th to the 97.5th percentile among the N sorted accuracy values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def bootstrap_distribution(Y_true, Y_pred, N):\n",
    "    \"\"\" \n",
    "    [TO BE IMPLEMENTED]\n",
    "    \n",
    "    Implement this function that takes the following inputs:\n",
    "    1. Y_true, Y_pred (list of true and predicted labels)\n",
    "    2. number of bootstrap trials (N). \n",
    "    \n",
    "    It should return a list (ret) of length = N, where ret[i] = accuracy metric from i-th bootstrap sampling run\n",
    "    \"\"\"\n",
    "    bootstrap_vals = [0]*N\n",
    "    return bootstrap_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "NUM_BOOTSTRAP = 1000\n",
    "\n",
    "bs_vals = bootstrap_distribution(Y_true, Y_pred, NUM_BOOTSTRAP)\n",
    "bs_vals = sorted(bs_vals)\n",
    "\n",
    "print(\"95 percent confidence interval: [{0}, {1}]\".format(\n",
    "    bs_vals[25],    # 2.5th percentile\n",
    "    bs_vals[975]    # 97.5th percentile\n",
    "))\n",
    "\n",
    "_ = plt.hist(bs_vals, bins='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Repeat the exact same run, but not only for data points with Y_true = 'Health'\n",
    "Do you see the confidence intervals narrower? wider? \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Y_true_health = []\n",
    "Y_pred_health = []\n",
    "\n",
    "bs_vals = bootstrap_distribution(Y_true_health, Y_pred_health, NUM_BOOTSTRAP)\n",
    "bs_vals = sorted(bs_vals)\n",
    "\n",
    "print(\"95 percent confidence interval: [{0}, {1}]\".format(\n",
    "    bs_vals[25],    # 2.5th percentile\n",
    "    bs_vals[975]    # 97.5th percentile\n",
    "))\n",
    "\n",
    "_ = plt.hist(bs_vals, bins='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral/Scenario based Tests\n",
    "\n",
    "Unit tests play an important role in testing software for bugs, inefficiencies and potential vulnerabilities. Can we employ a similar approach to testing ML models? This exercise introduces the concept of “behavioral testing”  for machine learning models. \n",
    "\n",
    "Minimum Functionality Tests are a class of behavioral tests, and equivalents of “unit tests in software engineering” - a collection of simple examples (and labels) to check a behavior. A recommended practice is to write minimum functionality tests for highly visible/high cost potential failure modes, and for failure modes that you uncover during error analysis to guard against such failures in the future. \n",
    "\n",
    "For this exercise, we will use a popular open source library called [Checklist](https://github.com/marcotcr/checklist) to configure and run behavioral tests for our model. The goal and scope of the exercise here is to get you acquainted with the library and practice of testing for minimum functionality. \n",
    "\n",
    "Useful references:\n",
    "1. Getting started with Checklist: https://github.com/marcotcr/checklist\n",
    "2. Creating & Running tests with Checklist: https://github.com/marcotcr/checklist/blob/9baab717e44e216697f7ef0730ee269db9ef7d5b/notebooks/tutorials/3.%20Test%20types,%20expectation%20functions,%20running%20tests.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.test_types import MFT\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "\n",
    "# Run some warmup code to get you familiar with templates in checlist\n",
    "editor = Editor()\n",
    "ret = editor.template('{first_name} is {a:profession} from {country}.',\n",
    "                      profession=['lawyer', 'doctor', 'accountant'])\n",
    "np.random.choice(ret.data, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "1. News Source variation:\n",
    "{source}: Astronomers expect the Perseid meteor shower to be one of the best versions of the shooting star events in several years\n",
    "source = ['New York Times', 'Reuters', 'AP', 'Wall Street Journal', 'Quanta', 'BBC', 'BBC UK', 'Yahoo News']\n",
    "label = \"Sci/Tech\"\n",
    "\n",
    "2. Company Name variation:\n",
    "{company} revealed Thursday that its old recipe of adding stores is longer is a source of new profits for the company.\n",
    "company = ['McDonalds', 'Starbucks', 'Chipotle', 'Krispy Creme', 'Unknown Company']\n",
    "label = \"Business\"\n",
    "\n",
    "3. Disease terms for healthcare news\n",
    "{disease} will come under the {mask} during the charity gala event being held on Monday at 7pm\n",
    "disease = ['Breast cancer', 'cancer', 'heart disease']\n",
    "label = \"Health\"\n",
    "\"\"\"\n",
    "\n",
    "editor = Editor()\n",
    "# [TO BE IMPLEMENTED]\n",
    "# ret = editor.template(...)\n",
    "# ret += editor.template(...)\n",
    "# ret += editor.template(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and run tests\n",
    "\n",
    "def encode_and_predict(inputs):\n",
    "    preds = [classifier.predict_label(x) for x in inputs]\n",
    "    print(preds)\n",
    "    return preds\n",
    "\n",
    "\n",
    "test = MFT(**ret, name='News classification behavioral tests')\n",
    "\n",
    "wrapped_pp = PredictorWrapper.wrap_predict(encode_and_predict)\n",
    "test.run(wrapped_pp, overwrite=True)\n",
    "test.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize model to prepare for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.dump('deploy/news_classifier.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d153e2a9af47b626c42d27093a83c0baabf3d339525f62429cac7540d67cb7fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
