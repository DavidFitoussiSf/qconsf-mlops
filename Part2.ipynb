{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the Part2 of this workshop, we will focus on model performance monitoring & feedback for the news classification model that we deployed\n",
    "\n",
    "1. We will download and parse the training dataset, logs from prediction service that record inference traffic and annotations (ground truth labels for the inference traffic)\n",
    "2. We will set up basic monitoring for system health (traffic volume, latency, SLA violations)\n",
    "3. We will compute data and label drift for the inference traffic using a few different techniques (Chi-square statistic, KS-statistic, classifier-based drift detection)\n",
    "4. We will understand model performance as a function of time for the inference traffic, and any ties we can derive to detected drift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "\n",
    "LABEL_SET = [\n",
    "    'Business',\n",
    "    'Sci/Tech',\n",
    "    'Software and Developement',\n",
    "    'Entertainment',\n",
    "    'Sports',\n",
    "    'Health',\n",
    "    'Toons',\n",
    "    'Music Feeds'\n",
    "]\n",
    "\n",
    "DATA_URL = 's3://corise-mlops/qconsf/dataset_part2.zip'\n",
    "\n",
    "LOG_DATE_START = date(2022, 10, 1)\n",
    "LOG_DATE_END = date(2022, 10, 14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Load Dataset\n",
    "\n",
    "In Part1, we worked with a modified version of the [AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) dataset - a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. This news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626). \n",
    "\n",
    "We started logging model inputs and predictions for the web application that we created for the trained news classifier. \n",
    "\n",
    "In Part2, We will work with the following files in the downloaded data:\n",
    "1. `training.json` -- this is the training data on which the classification model was trained. This will act as the reference dataset when we want to compute things like drift and outliers. Each row in this file is a training data point.\n",
    "2. `logs.json` -  this is a collection of logged outputs from our online service (the inference traffic). Each row in this file is a timestamped request, and contains the input request (text description, embedding, url etc) as well as model predictions. This will act as the target dataset when we want to compute things like drift and outliers. **The logs span a two week period from 2022/10/01 to 2022/10/14.**\n",
    "3. `annotations.json` - this is the set of ground truth labels for requests received by our online prediction service. Imagine we have a team of human annotators that label a fraction of our inference stream (with some delay). Eventually these ground truth labels are logged and will be helpful for us to monitor online model performance, and also is a good source of future training data for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample row in the logs file:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"id\": 69265,\n",
    "    \"timestamp\": \"2022-10-11 00:00:00\",\n",
    "    \"host_id\": \"host_4\",\n",
    "    \"latency_ms\": 169.676,\n",
    "    \"request\": {\n",
    "        \"id\": 69265,\n",
    "        \"source\": null,\n",
    "        \"title\": \"Google May Face Another Lawsuit\",\n",
    "        \"url\": \"http://www.pcworld.com/news/article/0,aid,117686,00.asp\",\n",
    "        \"rank\": \"5\",\n",
    "        \"description\": \"A federal judge in Virginia has ruled that a trademark infringement suit filed by the Government Employees Insurance Co. (GEICO) against Internet search giants Google and Overture Services can proceed.\",\n",
    "        \"embedding\": [...]\n",
    "    },\n",
    "    \"pred_label\": \"Business\",\n",
    "    \"pred_score\": {\n",
    "        \"Business\": 0.39581484916169474,\n",
    "        \"Entertainment\": 0.19195937955028541,\n",
    "        \"Health\": 0.02007952252798203,\n",
    "        \"Music Feeds\": 0.0005983183076385058,\n",
    "        \"Sci/Tech\": 0.3849374113779283,\n",
    "        \"Software and Developement\": 0.001389120851963045,\n",
    "        \"Sports\": 0.004566606342231832,\n",
    "        \"Toons\": 0.0006547918802761208\n",
    "    }\n",
    "}\n",
    "```\n",
    "Sample row in the annotations file (rows in here should be joined to the correct request in `logs.json` using the \"id\" field):\n",
    "```\n",
    "{\n",
    "    \"id\": 69265,\n",
    "    \"label\": \"Sci/Tech\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"\n",
    "    Download the dataset. The zip contains three files: train.json, test.json and unlabeled.json \n",
    "    \"\"\"\n",
    "    http_response = urlopen(DATA_URL)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path='data')\n",
    "\n",
    "# Expensive operation so we should just do this once\n",
    "download_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = pd.read_json('data/training.json', lines=True)\n",
    "logs = pd.read_json('data/logs.json', lines=True)\n",
    "true_labels = pd.read_json('data/annotations.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the training dataset\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the logged traffic\n",
    "logs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Health & Metrics\n",
    "\n",
    "Software system failures and downtimes are remarkably common even in ML systems. In an [analysis](https://www.youtube.com/watch?v=hBMHohkRgAA) conducted by Daniel Papasian and Todd Underwood (both ML engineers at Google), they looked at large ML pipeline failures at Google and found out that in 60 out of the 96 cases, the cause for failure was not directly related to the ML model.\n",
    "\n",
    "As a start, tracking system health can be a good first step to ensure properties such as whether the deployed model is available online, is its latency within acceptable SLAs, are the system resources (such as CPU and memory usage) within bounds etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Each row in the logs.json file has an associated timestamp of the format YYYY-MM-DD HH:MM:SS.\n",
    "We will use the timestamp to group requests by date, and track a few system metrics of interest:\n",
    "\n",
    "1. Volume: Compute the daily volume of requests received by our service, grouped by host id.\n",
    "2. Latency: Compute the mean, median, P90, P95 latency of requests received by our service, grouped by host id\n",
    "3. Plot each of these as a line chart, with date on the X-axis and the computed metric on the Y-axis. \n",
    "You can plot the metric for each host id in the same line chart (with different colors for each host).\n",
    "\n",
    "Do you notice anything strange with one of the hosts? :) \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Let us say that we have a maximum latency SLA of 300 milliseconds i.e. \n",
    "if the prediction service took more than 300 ms to answer the request, the downstream experience is degraded for users.\n",
    "\n",
    "1. Compute the aggregate daily volume of latency SLA violations (i.e. number of requests that have a latency >= maximum allowed latency) grouped by each host id \n",
    "2. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring model inputs and outputs\n",
    "\n",
    "In supervised machine learning settings, we assume that the training dataset D = {X, y} is composed of input/label tuples {Xi, yi} that are independently drawn from some underlying joint distribution ‚Ñô(X, y) such that ‚Ñô(X, y) = ‚Ñô(y|X)‚Ñô(ùêó)\n",
    " \n",
    "‚Ñô(y|X) is the relationship we are trying to learn during the model training step, which can then be used to generate accurate predictions for unseen samples. We make two assumptions here:\n",
    "1. The unseen samples that the model will be used to make predictions on, comes from the same underlying distribution ‚Ñô(X, y).\n",
    "2. This distribution ‚Ñô(X, y) is stationary and does not change with time.\n",
    "\n",
    "In practice, this assumption does not hold in most cases. Can we track and quantify this change over time though? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input drift (hypothesis testing)\n",
    "\n",
    "Hypothesis testing is a principled approach to evaluating drift. It‚Äôs a test to determine whether the difference between two populations (two sets of data) is statistically significant. If the difference is statistically significant, then the probability that the difference is a random fluctuation due to sampling variability is very low, and therefore, the difference is caused by the fact that these two populations come from two distinct distributions.\n",
    "\n",
    "1. Design a test statistic (or a distance metric) that is computed on samples collected form the two distributions - in our case, the reference and target distributions (i.e. data points from the inference and reference datasets)\n",
    "2. The test statistic is expected to be small if the null hypothesis is true (i.e. Z and Zref are drawn from the same distribution), and large if the alternative hypothesis (i.e. Z and Zref are drawn from different distributions) is true. \n",
    "3. From the test statistic, we compute a p-value: When p-value ‚â§ threshold, results from the test are said to be statistically significant, and the null hypothesis P(z) = Pref(z) is rejected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "News source (e.g. New York Times, Reuters, BBC etc) is an important piece of metadata about the incoming requests.\n",
    "Different news sources cover different topics, and any shift in the distribution/prevalence of news sources \n",
    "can be an important early signal to suggest that online traffic patterns might be changing. \n",
    "\n",
    "1. Both, the training dataset and infernce logs contains the news source metadata. \n",
    "2. Using the training dataset as the reference, quantify the drift in news sources for incoming requests. \n",
    "   You will compute drift for each day, using the inference logs from that day (i.e. \"target\") and comparing it to the training dataset (i.e. \"reference\")\n",
    "3. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
    "\n",
    "Chi-squared statistic is commonly used to measure drift for categorical features as we discussed.\n",
    "\n",
    "Refer to: \n",
    "1) https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html \n",
    "2) https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/chisquaredrift.html (in turn uses scipy under the hood)\n",
    "\n",
    "These implementations will return the distance, and the p-value of the null hypothesis as part of the response\n",
    "You want to plot the p-value in the line chart.\n",
    "\n",
    "MAKE SURE YOU DEAL WITH MISSING/NULL/NONE VALUES IN THE TRAINING AND INFERENCE DATA IN A CONSISTENT WAY\n",
    "\"\"\"\n",
    "\n",
    "news_sources_ref = [row['source'] for row in training]\n",
    "delta = LOG_DATE_END - LOG_DATE_START\n",
    "\n",
    "news_sources_drift = []\n",
    "\n",
    "for d in range(delta.days + 1):\n",
    "   curr_date = LOG_DATE_START + timedelta(days=d)\n",
    "   print(curr_date)\n",
    "\n",
    "   # Step 1: collect all requests from logs.json with timestamp on the curr_date\n",
    "   # this is currently a dummy empty list\n",
    "   news_sources_target = []\n",
    "\n",
    "   # Step 2: compute the drift between `news_sources_target` and `news_sources_ref` using Chi squared statistic\n",
    "   # you can use either the scipy.stats.chisquare or the implementation in alibi-detect as shared in the references above\n",
    "\n",
    "   # Step 3: store the pvalue in `news_sources_drift`\n",
    "\n",
    "\n",
    "# Step 4:  Plot the `news_sources_drift` as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [] # dates\n",
    "y = [] # per day drift value\n",
    "\n",
    "plt.plot(x, y, 'bo:')\n",
    "plt.xlabel(\"date\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input drift (Classifier based)\n",
    "\n",
    "The previous approach of treating drift detection as a two-sample hypothesis test is based on designing the correct test statistic. Oftentimes it can be hard to choose the correct statistic, and most test statistics are prone to false positives especially for multivate high dimensional data. \n",
    "\n",
    "What if we instead treat this as a classification problem? Can we train a classifier to predict which of the two distributions (reference or the target distribution) a given data point came from? The basic intuition is that if a classifier can learn to discriminate between the two distributions significantly better than random, then drift must have occurred.\n",
    "\n",
    "The classifier-based drift detector tries to correctly distinguish instances from the reference dataset vs. the target dataset by training a classifier. It is possible to consume outputs of the classifier in a few different ways. \n",
    "\n",
    "1. We can binarize the classifier prediction score based on a decision threshold, and apply a binomial test on the binarized predictions of the reference vs. the target data.\n",
    "2. We can use the classifier prediction score directly and compare the different in score distributions for the reference and target datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Embeddings represent an encoding of the semantic content of the inputs to the model, and also the feature space on \n",
    "top of which we train the classifier. Any shift in the space of embeddings can be an important signal to suggest \n",
    "that online traffic patterns might be changing which can impact downstream model performance\n",
    "\n",
    "1. Both, the training dataset and infernce logs contain embedding representations of the news article description.\n",
    "2. Using the training dataset as the reference, quantify the drift in embeddings for incoming requests. \n",
    "   You will compute drift for each day, using the inference logs from that day (i.e. \"target\") and comparing it to the training dataset (i.e. \"reference\")\n",
    "3. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
    "\n",
    "We will implement classifer-based drift detection for embeddings. You are free to choose the architecture of the classifier, \n",
    "but feel free to go with something simple (e.g. logistic regression, or a neural network with one hidden layer).\n",
    "\n",
    "Remember, this is a binary classifier that tries to predict whether a given data point belongs to the reference or the target distribution. \n",
    "\n",
    "Also, remember that a new instance of the classifier will be trained for each (referce, target) pair i.e. for each day\n",
    "\n",
    "Refer to: \n",
    "1) https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/classifierdrift.html\n",
    "2) https://openreview.net/forum?id=SJkXfE5xx \n",
    "\n",
    "These implementations will return the distance, and the p-value of the null hypothesis as part of the response\n",
    "You want to plot the p-value in the line chart.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "embeddings_ref = [row['embedding'] for row in training]\n",
    "\n",
    "delta = LOG_DATE_END - LOG_DATE_START\n",
    "embeddings_drift = []\n",
    "\n",
    "for d in range(delta.days + 1):\n",
    "   curr_date = LOG_DATE_START + timedelta(days=d)\n",
    "   print(curr_date)\n",
    "\n",
    "   # Step 1: collect all requests from logs.json with timestamp on the curr_date\n",
    "   # this is currently a dummy empty list\n",
    "   embeddings_target = []\n",
    "\n",
    "   # Step 2: compute the drift between `embeddings_target` and `embeddings_ref`\n",
    "   # (i) Initialize a new instance of the drift model (e.g. LogisticRegression())\n",
    "   # (ii) Initialize the drift detector (check out https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/classifierdrift.html)\n",
    "   # (iii) use the initialized drift detector to compute the p-value\n",
    "\n",
    "   # Step 3: store the pvalue in `embeddings_drift`\n",
    "\n",
    "\n",
    "# Step 4:  Plot the `embeddings_drift` as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []  # dates\n",
    "y = []  # per day drift value\n",
    "\n",
    "plt.plot(x, y, 'bo:')\n",
    "plt.xlabel(\"date\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "It seems like there is some divergence in the distributions of embeddings between the reference and target set on some days. \n",
    "Let's visualize with UMAP (https://umap-learn.readthedocs.io/en/latest/). Uniform Manifold Approximation and Projection (UMAP) \n",
    "is a dimension reduction technique that can be used for visualisation similarly to t-SNE. \n",
    "\n",
    "1. We will train a UMAP model to project our original embedding space into 2 dimensions, using our reference (training) dataset.\n",
    "2. We will then use this trained model to map the reference and the target dataset into two dimensions, and visualize it as a scatter plot. \n",
    "\n",
    "Do this exercise separately for two target distributions:\n",
    "1. inference traffic from the first day\n",
    "2. inference traffic from the last day\n",
    "\"\"\"\n",
    "\n",
    "import umap\n",
    "\n",
    "def train_umap_model(emb):\n",
    "    umap_model = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=5,\n",
    "        min_dist=0.1,\n",
    "        spread=2.0\n",
    "    )\n",
    "    umap_model.fit(emb)\n",
    "    return umap_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train UMAP on the reference data\n",
    "embeddings_ref = [row['embedding'] for row in training]\n",
    "umap_model = train_umap_model(embeddings_ref)\n",
    "umap_ref = umap_model.transform(embeddings_ref)\n",
    "\n",
    "# UMAP model inference\n",
    "logs_10_01 = logs[logs['date'] == LOG_DATE_START]['request'].tolist()\n",
    "embeddings_10_01 = [log['embedding'] for log in logs_10_01]\n",
    "umap_10_01 = umap_model.transform(embeddings_10_01)\n",
    "\n",
    "logs_10_14 = logs[logs['date'] == LOG_DATE_END]['request'].tolist()\n",
    "embeddings_10_14 = [log['embedding'] for log in logs_10_14]\n",
    "umap_10_14 = umap_model.transform(embeddings_10_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import plotly.express as px\n",
    "\n",
    "final_df = []\n",
    "\n",
    "for idx, ut in enumerate(sample(umap_ref.tolist(), 2000)):\n",
    "    final_df.append([ut[0], ut[1], 'reference'])\n",
    "\n",
    "for idx, ut in enumerate(sample(umap_10_01.tolist(), 2000)):\n",
    "    final_df.append([ut[0], ut[1], 'target(10/01)'])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    final_df, columns=[\"x\", \"y\", \"label\"]\n",
    ")\n",
    "\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\")\n",
    "fig.update_traces(marker={'size': 3})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import plotly.express as px\n",
    "\n",
    "final_df = []\n",
    "\n",
    "for idx, ut in enumerate(sample(umap_ref.tolist(), 2000)):\n",
    "    final_df.append([ut[0], ut[1], 'reference'])\n",
    "\n",
    "for idx, ut in enumerate(sample(umap_10_14.tolist(), 2000)):\n",
    "    final_df.append([ut[0], ut[1], 'target(10/01)'])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    final_df, columns=[\"x\", \"y\", \"label\"]\n",
    ")\n",
    "\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\")\n",
    "fig.update_traces(marker={'size': 3})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output (Label) Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "In this section, we will repeat the analysis we did in Step 4 [part 1], but for model predictions (outputs). \n",
    "\n",
    "A shift in the distribution of predicted labels and/or scores can be an important signal to suggest \n",
    "that online traffic patterns might be changing. \n",
    "\n",
    "1. The infernce logs contains the `pred_label` which is the predicted label of the model\n",
    "2. Using true labels from training dataset as the reference, quantify label drift for incoming requests. \n",
    "   You will compute drift for each day, using the inference logs from that day (i.e. \"target\") and comparing it to the training dataset (i.e. \"reference\")\n",
    "3. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
    "\n",
    "Chi-squared statistic is commonly used to measure drift for categorical features as we discussed.\n",
    "\n",
    "Refer to: \n",
    "1) https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html \n",
    "2) https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/chisquaredrift.html (in turn uses scipy under the hood)\n",
    "\n",
    "These implementations will return the distance, and the p-value of the null hypothesis as part of the response\n",
    "You want to plot the p-value in the line chart.\n",
    "\n",
    "Which labels are the most over-predicted compared to their prevalence in the training data? \n",
    "Which labels are the most under-predicted compared to their prevalence in the training data? \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "Ground truth observations (true labels) are a critical part of all supervised/self-supervised machine learning. Ground truth is important not just for model training, but also for monitoring and measuring performance of models after deployment.\n",
    "\n",
    "`annotations.json` is the set of ground truth labels for requests received by our online prediction service. Imagine we have a team of human annotators that label a fraction of our inference stream (with some delay). Eventually these ground truth labels are logged and will be helpful for us to monitor online model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Weekly classification report\n",
    "\n",
    "Compute the a classification report for each week of the logged inference data\n",
    "   Week 1 = [2022-10-01 and 2022-10-07]\n",
    "   Week 2 = [2022-10-08 and 2022-10-14]\n",
    "\n",
    "Do you notice a trend in model performance? \n",
    "How does this correlate to observed drift in news source, embeddings etc ? \n",
    "\"\"\"\n",
    "\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "print(sklearn_metrics.classification_report([], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TO BE IMPLEMENTED]\n",
    "\n",
    "Daily aggregate classification accuracy\n",
    "\n",
    "1. Compute the daily aggregate classification accuracy of the model\n",
    "2. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis. \n",
    "\n",
    "Do you notice a trend in model performance? \n",
    "How does this correlate to observed drift in news source, embeddings etc ? \n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d153e2a9af47b626c42d27093a83c0baabf3d339525f62429cac7540d67cb7fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
